{
    "id": "a92a32f7-ca13-4129-9f82-49fefc35329b",
    "created_at": "2026-01-13T13:54:31.502654+00:00",
    "updated_at": "2026-01-13T13:54:31.502654+00:00",
    "name": "De plus en plus de femmes interrogent l’IA sur leur santé : une étude alerte sur les urgences mal détectées",
    "language": "fr",
    "duration_seconds": "239",
    "transcript": "De plus en plus de femmes se tournent vers les chatbots d’intelligence artificielle pour poser des questions de santé, comprendre des symptômes ou décider s’il faut consulter. Ces outils, accessibles et perçus comme neutres, sont souvent utilisés comme un premier filtre avant un avis médical. Mais que valent-ils réellement lorsqu’il s’agit de situations potentiellement urgentes ? Une étude récente, publiée sur arXiv, menée par des chercheuses et cliniciennes spécialistes en santé des femmes, s’est penchée sur cette question sensible.\n\nLes conclusions soulèvent des interrogations majeures sur la fiabilité de ces outils dans un domaine où les omissions peuvent avoir des conséquences graves.\n\nDes tests pensés pour repérer les situations à risque\n\nPour évaluer les performances des chatbots, les chercheurs ont choisi une approche volontairement exigeante. Treize grands modèles d’intelligence artificielle ont été soumis à plusieurs centaines de questions médicales couvrant cinq spécialités, dont la médecine d’urgence, la gynécologie et la neurologie. Les scénarios, rédigés par des cliniciennes, pharmaciennes et chercheuses européennes et américaines, visaient surtout des situations où un retard ou une banalisation du symptôme peut poser problème.\n\nLes réponses ont ensuite été évaluées par ces mêmes expertes. Lorsque les modèles ne signalaient pas un risque sérieux ou n’orientaient pas clairement vers une prise en charge adaptée, la réponse était considérée comme insuffisante.\n\nSelon elles, environ 60 % des réponses fournies par les modèles ne répondaient pas aux exigences minimales d’un conseil médical fiable. GPT-5 est le modèle le plus performant de l’échantillon, avec environ 47 % d’échecs, contre 73 % pour Ministral 8B, qui enregistre les résultats les plus faibles.\n\nLes cas problématiques ont servi à constituer un benchmark spécifique à la santé des femmes, conçu comme un outil de référence pour tester les IA médicales.\n\n« Notre objectif n’était pas d’affirmer que les modèles sont globalement dangereux, mais de définir une norme d’évaluation clinique claire », explique Victoria-Elisabeth Gruber, membre de l’équipe de recherche. Elle souligne le cadre dit « volontairement conservateur », car en santé, « des omissions apparemment mineures peuvent avoir des conséquences selon le contexte ».\n\nUn problème structurel déjà observé dans d’autres études\n\nPlusieurs études récentes montrent que les systèmes d’IA ont tendance à minimiser ou mal contextualiser les symptômes féminins, en particulier lorsqu’ils ne correspondent pas aux présentations « classiques ».\n\nUne étude publiée en 2024 dans BMC Medical Informatics and Decision Making a, par exemple, montré que des modèles d’IA utilisés pour résumer des dossiers de soins sociaux pouvaient sous-évaluer la gravité de situations médicales concernant des femmes, influençant potentiellement les décisions de suivi.\n\nD’autres travaux, notamment en cardiologie, ont mis en évidence des différences de recommandations lorsque les mêmes symptômes sont présentés comme appartenant à un homme ou à une femme. Ces écarts reflètent un biais bien documenté en médecine humaine, biais dont a hérité l’IA.\n\nUn enjeu clé : l’IA comme outil de triage informel\n\nLe principal enjeu ne réside pas uniquement dans les erreurs factuelles, mais dans la façon dont l’intelligence artificielle est utilisée. Beaucoup de femmes ne cherchent pas un diagnostic précis, mais une réponse simple : est-ce grave ? Dois-je consulter maintenant ?\n\nDans ce rôle de premier filtre, l’essentiel n’est pas d’être parfaitement exact, mais de savoir repérer les signaux d’alerte et orienter vers un professionnel de santé en cas de doute.\n\nComme le résume Cara Tannenbaum, de l’Université de Montréal, ces résultats rappellent surtout « la nécessité de mettre à jour les contenus médicaux en ligne avec des données explicites liées au sexe et au genre ». Sans cela, l’IA risque de reproduire les mêmes angles morts que la médecine humaine, là où la vigilance devrait être maximale.",
    "audio_filename": "co-ce-practice/B2/a92a32f7-ca13-4129-9f82-49fefc35329b/audio.mp3",
    "audio_mime_type": "application/octet-stream"
}